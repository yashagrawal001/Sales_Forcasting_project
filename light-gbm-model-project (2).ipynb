{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport gc\nimport lightgbm as lgb\nimport joblib\nfrom lightgbm import LGBMRegressor\nfrom hyperopt import hp, tpe, fmin, Trials, rand, anneal","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nbase_path = \"/kaggle/input/m5-forecasting-accuracy/\"\ncalendar = pd.read_csv(f\"{base_path}calendar.csv\")\ntrain_eva = pd.read_csv(f\"{base_path}sales_train_evaluation.csv\")\nsell_prices = pd.read_csv(f\"{base_path}sell_prices.csv\")\nsample_sub = pd.read_csv(f\"{base_path}sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add more columns in file train\nfor d in range(1942,1970):\n    col = 'd_' + str(d)\n    train_eva[col] = 0\n    train_eva[col] = train_eva[col].astype(np.int16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Downcasting data\")\ntrain_eva = downcast(train_eva)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Melting data\")\ndf = pd.melt(frame=train_eva, \n             id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n             var_name=\"d\", value_name=\"sold\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Merging data\")\ndf = pd.merge(left=df, right=calendar, how=\"left\", on=\"d\")\ndf = pd.merge(left=df, right=sell_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Implement features\")\n#Calculate the SNAP (Supplemental Nutrition Assistance Program) day for each state\ndf[\"snap\"] = df[\"snap_CA\"] + df[\"snap_TX\"] + df[\"snap_WI\"]\ndf[\"snap\"] = np.where(df[\"snap\"] >= 1, 1, 0).astype(np.int8)\n\n# Apply int for day column\ndf[\"d\"] = df[\"d\"].str[2:].astype(np.int16)\n\n# Process NaN value\ndf[\"sell_price\"] = df['sell_price'].fillna(df.groupby('id')['sell_price'].transform('median'))\n\n# Is it a weekend\ndf[\"weekend\"] = np.where(df[\"wday\"] < 3, 1, 0).astype(np.int8)\n\n# Drop unnecessary columns\ndf = df.drop([\"date\", \"weekday\", \"wm_yr_wk\", \"event_name_2\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoder\nprint(\"Label Encoding\")\nd_id = dict(zip(df[\"id\"].cat.codes, df[\"id\"]))\nd_store = dict(zip(df[\"store_id\"].cat.codes, df[\"store_id\"]))\ndf[\"id\"] = df[\"id\"].cat.codes\ndf[\"item_id\"] = df[\"item_id\"].cat.codes\ndf[\"dept_id\"] = df[\"dept_id\"].cat.codes\ndf[\"cat_id\"] = df[\"cat_id\"].cat.codes\ndf[\"store_id\"] = df[\"store_id\"].cat.codes\ndf[\"state_id\"] = df[\"state_id\"].cat.codes\ndf[\"event_name_1\"] = df[\"event_name_1\"].cat.codes\ndf[\"event_type_1\"] = df[\"event_type_1\"].cat.codes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Calulating Lags and Rolling mean\")\n# Lags must be > 28\nlags = [29,30,31,32,33,34,35,40,55,60,65,180]\nfor lag in lags:\n    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)\n    \ndf['rolling_mean_7']   = df.groupby(['id'])['sold'].transform(lambda x: x.shift(28).rolling(7).mean())\ndf['rolling_mean_14']   = df.groupby(['id'])['sold'].transform(lambda x: x.shift(28).rolling(14).mean())\ndf['rolling_mean_30']  = df.groupby(['id'])['sold'].transform(lambda x: x.shift(28).rolling(30).mean())\ndf['rolling_mean_60']  = df.groupby(['id'])['sold'].transform(lambda x: x.shift(28).rolling(60).mean())\ndf['rolling_mean_180']  = df.groupby(['id'])['sold'].transform(lambda x: x.shift(28).rolling(180).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df[\"d\"] > 28+180]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dataframe\ndf.to_pickle('data.pkl')\ndel df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get from file Tuning HyperParams\nbest_params_store = (\n{'CA_1': {'colsample_bytree': 0.8,\n  'learning_rate': 0.2,\n  'max_depth': 7.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 200.0,\n  'subsample': 0.9},\n 'CA_2': {'colsample_bytree': 0.9,\n  'learning_rate': 0.30000000000000004,\n  'max_depth': 8.0,\n  'min_child_weight': 200.0,\n  'num_leaves': 200.0,\n  'subsample': 0.8},\n 'CA_3': {'colsample_bytree': 0.9,\n  'learning_rate': 0.2,\n  'max_depth': 8.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 250.0,\n  'subsample': 0.9},\n 'CA_4': {'colsample_bytree': 0.9,\n  'learning_rate': 0.30000000000000004,\n  'max_depth': 7.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 200.0,\n  'subsample': 0.9},\n 'TX_1': {'colsample_bytree': 0.8,\n  'learning_rate': 0.2,\n  'max_depth': 7.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 200.0,\n  'subsample': 1.0},\n 'TX_2': {'colsample_bytree': 1.0,\n  'learning_rate': 0.30000000000000004,\n  'max_depth': 8.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 200.0,\n  'subsample': 0.9},\n 'TX_3': {'colsample_bytree': 0.8,\n  'learning_rate': 0.2,\n  'max_depth': 7.0,\n  'min_child_weight': 300.0,\n  'num_leaves': 250.0,\n  'subsample': 0.9},\n 'WI_1': {'colsample_bytree': 0.9,\n  'learning_rate': 0.2,\n  'max_depth': 8.0,\n  'min_child_weight': 400.0,\n  'num_leaves': 250.0,\n  'subsample': 0.8},\n 'WI_2': {'colsample_bytree': 1.0,\n  'learning_rate': 0.2,\n  'max_depth': 8.0,\n  'min_child_weight': 200.0,\n  'num_leaves': 250.0,\n  'subsample': 0.9},\n 'WI_3': {'colsample_bytree': 0.9,\n  'learning_rate': 0.2,\n  'max_depth': 8.0,\n  'min_child_weight': 400.0,\n  'num_leaves': 250.0,\n  'subsample': 0.9}})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validation(data, valid_first_day, params, day_start, d_store=d_store, sample_sub=sample_sub):\n    \n    valid = data[(data['d']>=valid_first_day) & (data['d']<valid_first_day+28)][['id','d','sold']]\n    print(f\"Valid first day {valid_first_day} predicting\")\n    for i in range(10): \n        # Forecast cho tá»«ng store \n        df = data[data[\"store_id\"] == i]\n\n        #Create train set\n        X_train, y_train = df[(df['d']>=day_start) & (df['d']<valid_first_day)].drop('sold',axis=1), df[(df['d']>=day_start) & (df['d']<valid_first_day)]['sold']\n        train_sets = lgb.Dataset(X_train, y_train)\n        X_valid, y_valid = df[(df['d']>=valid_first_day) & (df['d']<valid_first_day+28)].drop('sold',axis=1), df[(df['d']>=valid_first_day) & (df['d']<valid_first_day+28)]['sold']\n        valid_sets = lgb.Dataset(X_valid, y_valid)\n\n        model = lgb.train(params={'objective' : 'tweedie',\n                                  'force_row_wise': True,\n                                  'verbose': -1,\n                                  'n_estimators': 1000,\n                                  'learning_rate':params[d_store[i]][\"learning_rate\"],\n                                  'subsample': params[d_store[i]][\"subsample\"],\n                                  'colsample_bytree':params[d_store[i]][\"colsample_bytree\"],\n                                  'min_child_weight':params[d_store[i]][\"min_child_weight\"],\n                                  'max_depth':np.int16(params[d_store[i]][\"max_depth\"]),\n                                  'num_leaves':np.int16(params[d_store[i]][\"num_leaves\"])},\n                                \n                      train_set=train_sets, \n                      valid_sets=valid_sets,\n                      verbose_eval=False,\n                      early_stopping_rounds=50)\n\n        pred_val = model.predict(X_valid)\n        valid.loc[X_valid.index, \"sold\"] = pred_val\n\n    valid[\"id\"] = valid[\"id\"].map(d_id)\n    valid = valid.pivot(index=\"id\", columns=\"d\", values=\"sold\").reset_index()\n    valid[\"id\"] = valid[\"id\"].str.replace(\"evaluation\", \"validation\")\n    \n    sample_sub = sample_sub[[\"id\"]]\n\n    f_col = [f\"F{i}\" for i in range(1,29)]\n    f_col.insert(0, \"id\")\n    \n    print(f\"Valid testset from day {valid.columns[1]} to day {valid.columns[-1]}\")\n    \n    out_val = pd.merge(left=sample_sub[:30490], right=valid, on=\"id\")\n    out_val.columns=f_col\n    \n    return out_val\n\ndef avg_rmsse_score(out_val, train_eva, valid_first_day, day_start):\n    \n    print(f\"Scoring from {valid_first_day} to {valid_first_day+28-1}\")\n    print(f\"Naive first day {day_start}\")\n    \n    days_train = [i for i in range(day_start, valid_first_day)]\n    days_valid = [i for i in range(valid_first_day, valid_first_day+28)]\n    \n    naive_predict = np.array(train_eva[days_train].drop(valid_first_day-1, axis=1)).astype(np.int32)\n    y_true_naive = np.array(train_eva[days_train].drop(day_start, axis=1)).astype(np.int32)\n    naive_mse = np.mean((naive_predict - y_true_naive) ** 2, axis=1)\n    \n    model_pred = np.array(out_val.iloc[:, 1:])\n    y_true_model = np.array(train_eva[days_valid])\n    model_mse = np.mean((model_pred - y_true_model) ** 2, axis=1)\n    \n    avg_rmsse = np.sqrt(model_mse / naive_mse).mean()\n    \n    return avg_rmsse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_eva.columns = list(train_eva.columns[:6]) + [i for i in range(1, 1970)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_first_days_list = [1858, 1886, 1914]\ncv_score = dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvalid_first_days_list = [1858, 1886, 1914]\nday_start = 209\nday_start_naive = 1\ncv_score = dict()\nfor i in valid_first_days_list:\n    out_df_cv = cross_validation(data=data, valid_first_day=i, day_start=day_start, params=best_params_store)\n    cv_score[i] = avg_rmsse_score(out_val=out_df_cv, train_eva=train_eva, valid_first_day=i, day_start=day_start_naive)\n    day_start += 28\n    day_start_naive += 28","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in valid_first_days_list:\n    print(f\"Score {i}\", cv_score[i])\n    \nprint(\"CV score\", np.mean(list(cv_score.values())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}